## Can we avoid syntax?

Without question, syntax is very important: people love and hate languages based upon it. But when designing a language, can we not just make it consumable to different syntaxes? Deep question, because syntax also informs how one should write a program in the first place: it guides you in its implicit ergonomics to certain patterns, and then language culture (and past influences) guide when you opt to move towards paths of higher resistance. This flow towards ease is vital to recognize, and syntax is a major player in this flow. Ultimately, I would like to design a language that supports syntax backends, and focuses on its own semantics. Users are free to specialize syntaxes that support their idioms and styles: the underlying language semantics will be preserved.

One area that troubles this though is what I might call (perhaps incorrectly, technically speaking) the calling conventions of your language. For example: does your method return Self or ()? If (), then it isn't ammenable to chaining syntax. In Rust style, there is a design sense for when to use either (typically, we don't try and support chaining for operations whose only point is to mutate, like push). But lets say we wrote a different syntax for Rust and had ergonomcis that really liked to chain all of the time, well, now we'd be returning Self in our methods everywhere, and we'd have a disruption of the other "languages" calling conventions.

I might suggest a solution to this particular problem, and offer it also as an example of how we might always want to figure out how our implementation of anything that diverges from our host can still be made amenable to it. In this case

### Broadening allowed function inputs through interfaces

Can we create functions that just allow anyone to specify their own types for varags, or for sequential outputs? Vectors, fixed arrays, linked lists, sets, all of these work perfectly fine in these cases so long as we have a common interface for getting the next element. With Rust, we like to use iterators to allow for this level of variance, but note that they are often less preferred than Vec because of the heavier ergonomics of iterators. In our language though, why not always expect varargs and sequential outputs to be a generic interface by default, and allow any of the aforementioned (and future) structures to be used for it? The cost is that we give up our specialized knowledge over how these data structures operate within our implementation. For example, testing set membership is loads cheaper than list membership. A way to perhaps circumvent this is to *know* all of the interface methods a function calls within it (and all the way down the call stack), and allow the user to know those values at both compile and run-time. Then they can use their specialized knowledge to make decisions about which data structure is most suited for their needs.

This is a very interesting area to consider. There are obvious cases where the only thing that makes sense is specialized algorithms, and of course, there should always be the ability to specialize more performance in orthogonally (without mucking up our clean, high-level defaults). But I like this idea of being less opinionated over which data structure makes sense. I don't want end-users to have to marshall their data into a Vec just because a Vec is the most performant for a given algorithm. There is an interesting inversion of OOP dispatch here: instead of our data structures each having specialized methods that solve our algorithm, meaning it can be seamlessly used on any of them, we're saying that all of our Sequences have a generic interface that we will code our algorithm to. This too is of course polymorphic dispatch. I really like this notion, though I have some hunches for ways this might muck with things.

#### Possible pitfalls, and idea for computer-assisted complexity-analysis

The one on my mind is Set, which has a semantic invariant that Lists, arrays, tuples, Vecs, Strings (we'll have to revisit that one too in a second) do not. The lack of duplication means that some operations will make no sense on a Set. But, although that may be true, given a generic interface, I doubt that this means that Set will *break* any algorithm. Instead, we as humans might just be able to statically know that an algorithm will perhaps be a no-op on a set: imagine, for example, a generic de-dup function: well, clearly by running this code on Set, it'll only waste processor cycles. Of course we could add in a specialization for Set to avoid this, but we'll always have orthogonal ways to specialize and tweak performance, so that's less important to me. I think that, in the end, not being opinionated here is good, even when certain data types might be really confusing or suboptimal for a problem. Hmmm, well, unless of course we could get unexpected exponential behavior with a given data type. Interestingly, we might just be able to bake in some complexity-time analysis within the language. Imagine that every data structures implementation of the sequential interface specified complexity times for each method. Then, a function using that interface could report its total complexity time estimate (likely we'd have upper and lower bounds). A programmer could, either dynamically or statically, set thresholds for complexity, and be given a compiler error if a data structure in a given computational flow would exceed those bounds. That sounds like a very fun and interesting problem.

### Generic interfaces and good-citizen calling conventions

Revisiting the idea of making sure our calling conventions are amenable to our host, what would the implications of this Sequence Interface pattern be on generated Rust code? Would we use Iterator for every argument? Perhaps we'd be able to offer all of an iterator interface, a generic function with Sequence trait bound (letting them just pass a Vec/slice for example), and specialized outputs for each type that explicitly bakes-in the code. These could be handled through feature flags as well, letting library consumers pick the amount of binary they welcome.

### Generic interfaces and how other languages deal

Clojure, and many other functional languages, generally stick to just a few core collection types. OOP is where we get an explosion of possibilities, where you can just inherit a vec and say you're a vec, and Python, Ruby, Java, etc... all represent languages which are open to being used this way. Inheritance is a heavier-handed version of what I'm saying too: it gives less control and predictability, but in the end it means that you can use your *own* type in place of Vec, by making it a Vec. JavaScript is the biggest dirth, where it's just gonna be an array, it's just gonna. Going back to clojure though, it does actually tend to convert everything to its ISequence interface, and then we have to call a terminating method on it to reclaim a concrete value out of this lazy stream. Rust uses iterators to more explicitly tell you: this stuff will fuse together into a one pass operation or so help me.

In the end, Rust isn't bad to want us to use Vecs everywhere for sequential data, it makes a huge amount of sense. Yet, it doesn't quite make any more sense than asking for <V>(seq: impl Iterator<Item=V>) in a function or method body. Why isn't there a Trait for sequences that have a known size? Yet another limitation, because methods like u32::to_le_bytes gives us [u8; 4], but not SizedSequence<u8, N=4>, where it could be anything. Now the mental overhead of a generic instead of a specific is meaningful, there is simplicity to be had at looking at [u8; 4]. And Rust, lang that it is, likely doesn't care that much about you not being able to return to_le_bytes as a linked list by default without just asking you to do it yourself. But there's something deliciously fun about saying "yes" to that, and the most obvious ergonomic cost is now suddenly needing to provide a return type when before it was pre-ordained. Type inference can help, but not certainly. I think SizedSequence is a super cool idea, it would have its own nice methods, and it would of course have all Sequence methods on it (but giving a compiler warning if it could statically known to not have sensible behavior, like calling .last() on a zero-sized sequence, which you should basically just be told: look this is just going to be None, don't waste your time calling it), and it would *downgrade* to a Sequence for any methods that could alter its size, barring static verification that they couldn't.

This gives us more flexibility, but at the cost of not just being able to say [u8; 4]. So there's something to be said about being able to view code as specialized versions, even if the implementation is written to be generic. I think that this is yet another example of why textfile oriented code is vestigial: the way we read our code should be dynamic. Sure, the source file will be SizedSequence<..>, but you should be viewing it in an environment that lets you just read [u8; 4] if your user options have been configured as such. Why not, right? Likewise, why not allow a user to auto-generate a Sequence implementation from their manual [u8; 4] implementation, giving an error if any non-sequence methods/functions were used on the [u8; 4], which is another way of saying: uh, maybe this shouldn't be allowed to be generic. In fact, why not make extraction a default, and then just give different analytics and runtime stuff to the user so that they can always, ergonomically, do the right thing, like telling the compiler to never allow this code to be generic because, for example, you've guaranteed that it will have certain performance chars that are important to the code (to which the compiler could just say: yeah sure of course but are you *sure* we can't just let your code be a specialization, proper docs and all? Would it be *harmful* to allow it to be generic? We could even promote your method as being the preferred way to use this function via docs, so that end-users know. We could *even* have a low-tier warning on the compiler for users who call functions that have a recommended specialization, and just request that they acknowledge that they don't want to before suppressing the warning; non-intrusive (and even fully hideable via user configuration) code would then be created to mark this specialization violate as fine, perhaps as an attribute on the containing function itself, if not inlined above). Wow, that was a long parenthetical.

### Rust nudges you to specialize too early

I think the main takeaway of all of that for me is to acknowledge that there's a whole ton we can do to make generic interfaces just as ergonomic as specialized ones, including things like default return types when we don't have enough context (again, that basically makes it as if you'd just written the specific version in the first place, without closing off). The thing with Rust is that it's very closed off to changing things. It is a lot of things, very secure, very predictable, utterly wonderful, but it will also gladly make you write more verbose, less generic code whenever it wants you to. The type of openness I just described requires quite a bit more work in Rust. And at the end of the day, if a function takes a Vec as an argument, you better give it a Vec: you can't change that, the decision was made for you. Again, sometimes this makes sense, sometimes we need structure-specific methods, but truly, largely speaking, we don't, and I don't see why practically all of this can't be auto-done by the compiler.

## First-class Rust support

Creating a language in Rust should extend use-cases rather than fragment them. In writing a compiled language, I believe compiling to Rust is an important goal. By doing this, we prepare ourselves for working on code output that can provide idiomatic Rust interfaces, irrespective of the design style of our language. This means development within this language not only fully integrates within the Rust ecosystem, but can actually further it in ways the whole ecosystem can benefit from.